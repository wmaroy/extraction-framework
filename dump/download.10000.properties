# NOTE: format is not java.util.Properties, but org.dbpedia.extraction.dump.download.DownloadConfig

# Default download server. It lists mirrors which may be faster. 
base=http://dumps.wikimedia.org/

# Replace by your target folder.
dir=/path/to/download/folder

# Needed when downloading languages based on article count
csv=http://s23.org/wikistats/wikipedias_csv

# Download dump for all languages with >= 10000 articles
dump=10000-:pages-articles.xml.bz2

# Only needed for the ImageExtractor
dump=commons:pages-articles.xml.bz2

# If you want to keep files zipped, delete the following line.
unzip=true

# Sometimes connecting to the server fails, so we try five times with pauses of 10 seconds.
retry-max=5
retry-millis=10000

# Only needed by the AbstractExtractor
dump=10000-:image.sql.gz,imagelinks.sql.gz,langlinks.sql.gz,templatelinks.sql.gz,categorylinks.sql.gz

# The following lines do not work yet. TODO: extend downloader, let us rename files.
# Wikipedia uses a patched MediaWiki version
# other=tables-1.19wmf1.sql=http://svn.wikimedia.org/svnroot/mediawiki/branches/wmf/1.19wmf1/maintenance/tables.sql
# other=tables-1.20wmf1.sql=https://gerrit.wikimedia.org/r/gitweb?p=mediawiki/core.git;a=blob_plain;f=maintenance/tables.sql;hb=wmf/1.20wmf1
